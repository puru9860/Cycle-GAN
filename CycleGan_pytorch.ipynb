{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CycleGan pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VEJrcfZuJyh4km2vUfzmoq2ii-hwOBwU",
      "authorship_tag": "ABX9TyNes05iedGAwG5Q/Rslktgg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puru9860/Cycle-GAN/blob/main/CycleGan_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hKDM3JbVwx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a384a48b-5c5a-46c7-b525-cca1e773f706"
      },
      "source": [
        "!pip install git+https://github.com/albumentations-team/albumentations.git\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import copy\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albumentations-team/albumentations.git\n",
            "  Cloning https://github.com/albumentations-team/albumentations.git to /tmp/pip-req-build-yy3q7cwi\n",
            "  Running command git clone -q https://github.com/albumentations-team/albumentations.git /tmp/pip-req-build-yy3q7cwi\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (0.16.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (4.1.2.30)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.5.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (3.2.2)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations==1.0.0) (4.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (1.15.0)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-1.0.0-cp37-none-any.whl size=98151 sha256=cc89e907d24e59bb5d586bc205a8e7f4c424f71df7d9071e62819cd7883a96aa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i2f6ma8l/wheels/e2/85/3e/2a40fac5cc1f43ced656603bb2fca1327b30ec7de1b1b66517\n",
            "Successfully built albumentations\n",
            "Installing collected packages: albumentations\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NaiI71xmGXL"
      },
      "source": [
        "# !pip install -U git+https://github.com/albu/albumentations > /dev/null "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLo2ApVryhsl"
      },
      "source": [
        "DEVICE  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TRAIN_DIR= \"/content/drive/MyDrive/unmask_2/train/\"\n",
        "VAL_DIR = \"/content/drive/MyDrive/unmask_2/val/\"\n",
        "BATCH_SIZE = 1\n",
        "LEARNING_RATE = 1e-5\n",
        "LAMBDA_IDENTITY = 0.0\n",
        "LAMBDA_CYCLE = 10\n",
        "NUM_WORKERS = 2\n",
        "NUM_EPOCHS = 1000\n",
        "LOAD_MODEL = True\n",
        "SAVE_MODEL = False\n",
        "CHECKPOINT_GEN_M = \"/content/drive/MyDrive/unmask_2/ck/genM.pth.tar\"\n",
        "CHECKPOINT_GEN_W = \"/content/drive/MyDrive/unmask_2/ck/genW.pth.tar\"\n",
        "CHECKPOINT_CRITIC_M = \"/content/drive/MyDrive/unmask_2/ck/criticM.pth.tar\"\n",
        "CHECKPOINT_CRITIC_W = \"/content/drive/MyDrive/unmask_2/ck/criticW.pth.tar\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMzBdNyRV4Yv"
      },
      "source": [
        "# Discriminator Block\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,stride):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels,4,stride,1,bias=True,padding_mode=\"reflect\"),\n",
        "        nn.InstanceNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self,in_channels=3,features=[64,128,256,512]):\n",
        "    super().__init__()\n",
        "    self.initial = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels,\n",
        "            features[0],\n",
        "            kernel_size=4,\n",
        "            stride=2,\n",
        "            padding=1,\n",
        "            padding_mode=\"reflect\"            \n",
        "        ),\n",
        "        nn.LeakyReLU(0.2),\n",
        "    )\n",
        "\n",
        "    layers = []\n",
        "    in_channels = features[0]\n",
        "    for feature in features[1:]:\n",
        "      layers.append(Block(in_channels,feature,stride=1 if feature == features[-1] else 2))\n",
        "      in_channels = feature\n",
        "    layers.append(nn.Conv2d(in_channels,1,kernel_size=4,stride=1,padding=1,padding_mode=\"reflect\"))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.initial(x)\n",
        "    return torch.sigmoid(self.model(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7ziMau7CzTE"
      },
      "source": [
        "# Generator model\n",
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels, down=True,use_act=True, **kwargs):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels,padding_mode=\"reflect\", **kwargs)\n",
        "        if down\n",
        "        else nn.ConvTranspose2d(in_channels,out_channels,**kwargs),\n",
        "        nn.InstanceNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.conv(x)\n",
        "  \n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self,channels):\n",
        "    super().__init__()\n",
        "    self.block = nn.Sequential(\n",
        "        ConvBlock(channels,channels,kernel_size=3,padding =1),\n",
        "        ConvBlock(channels,channels,use_act=False,kernel_size=3,padding=1),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return x + self.block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self,img_channels,num_features=64,num_residuals=9):\n",
        "    super().__init__()\n",
        "    self.initial = nn.Sequential(\n",
        "      nn.Conv2d(img_channels,num_features,kernel_size=7,stride = 1,padding=3,padding_mode=\"reflect\"),\n",
        "      nn.ReLU(inplace=True),\n",
        "    )\n",
        "    self.down_blocks = nn.ModuleList(\n",
        "        [\n",
        "          ConvBlock(num_features,num_features*2,kernel_size = 3,stride =2,padding=1),\n",
        "          ConvBlock(num_features*2,num_features*4,kernel_size = 3,stride =2,padding=1),\n",
        "\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.residual_blocks = nn.Sequential(\n",
        "        *[ResidualBlock(num_features*4) for _ in range(num_residuals)]\n",
        "    )\n",
        "\n",
        "    self.up_blocks = nn.ModuleList(\n",
        "        [\n",
        "         ConvBlock(num_features*4,num_features*2,down=False,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
        "         ConvBlock(num_features*2,num_features,down=False,kernel_size=3,stride=2,padding=1,output_padding=1)\n",
        "\n",
        "        ]\n",
        "    )\n",
        "    self.last = nn.Conv2d(num_features,img_channels,kernel_size=7,stride=1,padding=3,padding_mode=\"reflect\")\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.initial(x)\n",
        "    for layer in self.down_blocks:\n",
        "      x = layer(x)\n",
        "\n",
        "    x = self.residual_blocks(x)\n",
        "\n",
        "    for layer in self.up_blocks:\n",
        "      x = layer(x)\n",
        "    return torch.tanh(self.last(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjvBa_fHEPRn"
      },
      "source": [
        "transforms = A.Compose(\n",
        "    [\n",
        "     A.Resize(width=256,height=256),\n",
        "     A.HorizontalFlip(p=0.5),\n",
        "     A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],max_pixel_value = 255),\n",
        "     ToTensorV2(),\n",
        "    ],\n",
        "    additional_targets={\"image0\":\"image\"},\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckHvPbESdIVI"
      },
      "source": [
        "class MaskedDataset(Dataset):\n",
        "  def __init__(self,root_masked,root_without,transform=None):\n",
        "    self.root_masked = root_masked\n",
        "    self.root_without  = root_without\n",
        "    self.transform = transform\n",
        "\n",
        "    self.masked_images = os.listdir(root_masked)\n",
        "    self.without_images = os.listdir(root_without)\n",
        "    self.masked_len = len(self.masked_images)\n",
        "    self.without_len = len(self.without_images)\n",
        "    self.length_dataset = max(self.masked_len,self.without_len)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length_dataset\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    masked_img = self.masked_images[index % self.masked_len]\n",
        "    without_img = self.without_images[index % self.without_len]\n",
        "\n",
        "    masked_path = os.path.join(self.root_masked,masked_img)\n",
        "    without_path = os.path.join(self.root_without,without_img)\n",
        "\n",
        "    masked_img = np.array(Image.open(masked_path).convert(\"RGB\"))\n",
        "    without_img = np.array(Image.open(without_path).convert(\"RGB\"))\n",
        "\n",
        "    if self.transform:\n",
        "      augmentations = self.transform(image=masked_img,image0=without_img)\n",
        "      masked_img = augmentations[\"image\"]\n",
        "      without_img = augmentations[\"image0\"]\n",
        "    \n",
        "    return masked_img,without_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af1_4N0bGKqG"
      },
      "source": [
        "# save and load checkpoints\n",
        "\n",
        "def save_checkpoint(model,optimizer,filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"=>Saving checkpoint\")\n",
        "  checkpoint = {\n",
        "      \"state_dict\":model.state_dict(),\n",
        "      \"optimizer\": optimizer.state_dict(),\n",
        "  }\n",
        "  torch.save(checkpoint,filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file,model,optimizer,lr):\n",
        "  print(\"=>Loading Checkpoint\")\n",
        "  checkpoint = torch.load(checkpoint_file,map_location=DEVICE)\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group[\"lr\"] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSmWt8zqHuMk"
      },
      "source": [
        "def train_fn(disc_M,disc_W,gen_M,gen_W,loader,opt_disc,opt_gen,L1,mse,d_scaler,g_scaler):\n",
        "  W_reals = 0\n",
        "  W_fakes = 0\n",
        "  loop = tqdm(loader,leave=True)\n",
        "\n",
        "  for idx , (masked,without) in enumerate(loop):\n",
        "    masked = masked.to(DEVICE)\n",
        "    without = without.to(DEVICE)\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "      fake_without = gen_W(masked)\n",
        "      D_W_real = disc_W(without)\n",
        "      D_W_fake = disc_W(fake_without.detach())\n",
        "      W_reals += D_W_real.mean().item()\n",
        "      W_fakes += D_W_fake.mean().item()\n",
        "      D_W_real_loss = mse(D_W_real,torch.ones_like(D_W_real))\n",
        "      D_W_fake_loss = mse(D_W_fake,torch.ones_like(D_W_fake))\n",
        "      D_W_loss = D_W_real_loss + D_W_fake_loss\n",
        "\n",
        "      fake_masked = gen_M(without)\n",
        "      D_M_real = disc_M(masked)\n",
        "      D_M_fake = disc_M(fake_masked.detach())\n",
        "      D_M_real_loss = mse(D_M_real, torch.ones_like(D_M_real))\n",
        "      D_M_fake_loss = mse(D_M_fake, torch.zeros_like(D_M_fake))\n",
        "      D_M_loss = D_M_real_loss + D_M_fake_loss\n",
        "\n",
        "      D_loss = (D_W_loss + D_M_loss)/2\n",
        "    \n",
        "    opt_disc.zero_grad()\n",
        "    d_scaler.scale(D_loss).backward()\n",
        "    d_scaler.step(opt_disc)\n",
        "    d_scaler.update()\n",
        "\n",
        "    # Train Generators H and Z\n",
        "    with torch.cuda.amp.autocast():\n",
        "        # adversarial loss for both generators\n",
        "        D_W_fake = disc_W(fake_without)\n",
        "        D_M_fake = disc_M(fake_masked)\n",
        "        loss_G_W = mse(D_W_fake, torch.ones_like(D_W_fake))\n",
        "        loss_G_M = mse(D_M_fake, torch.ones_like(D_M_fake))\n",
        "\n",
        "        # cycle loss\n",
        "        cycle_masked = gen_M(fake_without)\n",
        "        cycle_without = gen_W(fake_masked)\n",
        "        cycle_masked_loss = L1(masked, cycle_masked)\n",
        "        cycle_without_loss = L1(without, cycle_without)\n",
        "\n",
        "        # identity loss (remove these for efficiency if you set lambda_identity=0)\n",
        "        identity_masked = gen_M(masked)\n",
        "        identity_without = gen_W(without)\n",
        "        identity_masked_loss = L1(masked, identity_masked)\n",
        "        identity_without_loss = L1(without, identity_without)\n",
        "\n",
        "        # add all togethor\n",
        "        G_loss = (\n",
        "            loss_G_M\n",
        "            + loss_G_W\n",
        "            + cycle_masked_loss * LAMBDA_CYCLE\n",
        "            + cycle_without_loss * LAMBDA_CYCLE\n",
        "            + identity_without_loss * LAMBDA_IDENTITY\n",
        "            + identity_masked_loss * LAMBDA_IDENTITY\n",
        "        )\n",
        "\n",
        "    opt_gen.zero_grad()\n",
        "    g_scaler.scale(G_loss).backward()\n",
        "    g_scaler.step(opt_gen)\n",
        "    g_scaler.update()\n",
        "\n",
        "    if idx % 200 == 0:\n",
        "        save_image(fake_without*0.5+0.5, f\"/content/drive/MyDrive/unmask_2/ck/without_{idx}.png\")\n",
        "        save_image(fake_masked*0.5+0.5, f\"/content/drive/MyDrive/unmask_2/ck/masked_{idx}.png\")\n",
        "\n",
        "    loop.set_postfix(W_real=W_reals/(idx+1), W_fake=W_fakes/(idx+1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWzYaCC3bOsc"
      },
      "source": [
        "disc_M = Discriminator(in_channels=3).to(DEVICE)\n",
        "disc_W = Discriminator(in_channels=3).to(DEVICE)\n",
        "gen_M = Generator(img_channels=3,num_residuals=9).to(DEVICE)\n",
        "gen_W = Generator(img_channels=3,num_residuals=9).to(DEVICE)\n",
        "\n",
        "opt_disc = optim.Adam(\n",
        "    list(disc_M.parameters()) + list(disc_W.parameters()),\n",
        "    lr = LEARNING_RATE,\n",
        "    betas = (0.5,0.999)\n",
        ")\n",
        "opt_gen = optim.Adam(\n",
        "    list(gen_M.parameters()) + list(gen_W.parameters()),\n",
        "    lr = LEARNING_RATE,\n",
        "    betas = (0.5,0.999)\n",
        ")\n",
        "\n",
        "L1 = nn.L1Loss()\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "if LOAD_MODEL:\n",
        "  load_checkpoint(\n",
        "      CHECKPOINT_GEN_M,gen_M,opt_gen,LEARNING_RATE,\n",
        "  )\n",
        "  load_checkpoint(\n",
        "      CHECKPOINT_GEN_W,gen_W,opt_gen,LEARNING_RATE,\n",
        "  )\n",
        "  load_checkpoint(\n",
        "      CHECKPOINT_CRITIC_M,disc_M,opt_disc,LEARNING_RATE,\n",
        "  )\n",
        "  load_checkpoint(\n",
        "      CHECKPOINT_CRITIC_W,disc_W,opt_disc,LEARNING_RATE,\n",
        "  )\n",
        "\n",
        "dataset = MaskedDataset(root_masked=TRAIN_DIR+\"masked\",root_without=TRAIN_DIR+\"without\",transform=transforms)\n",
        "\n",
        "val_dataset =  MaskedDataset(root_masked=VAL_DIR+\"masked\",root_without=VAL_DIR+\"without\",transform=transforms)\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle = True,\n",
        "    num_workers = NUM_WORKERS,\n",
        "    pin_memory = True\n",
        ")\n",
        "g_scaler = torch.cuda.amp.GradScaler()\n",
        "d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"\\nepoch: {epoch}\")\n",
        "  train_fn(disc_M,disc_W,gen_M,gen_W,loader,opt_disc,opt_gen,L1,mse,d_scaler,g_scaler)\n",
        "\n",
        "  if SAVE_MODEL and epoch % 10 ==0  :\n",
        "    save_checkpoint(gen_M,opt_gen,filename=CHECKPOINT_GEN_M)\n",
        "    save_checkpoint(gen_W,opt_gen,filename=CHECKPOINT_GEN_W)\n",
        "    save_checkpoint(disc_M,opt_disc,filename=CHECKPOINT_CRITIC_M)\n",
        "    save_checkpoint(disc_W,opt_disc,filename=CHECKPOINT_CRITIC_W)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drmm3EcX8HUf"
      },
      "source": [
        "masked_img = np.array(Image.open('/content/h1.jpg').convert(\"RGB\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NnfC2TCC4Hu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}